{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Notebook hecho por Victoria)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PT12 - Proyecto final - Grupo 2\n",
    "#### Una notebook para todo. \n",
    "##### Solo falta resolver lo de las dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leo el archivo parquet datos crudos\n",
    "# How to read a Parquet file into Pandas DataFrame?\n",
    "gm_sitios = pd.read_parquet(\"datos/gm_raw_parquet/gm_sitios_raw.parquet\" , engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas que no serán utilizadas\n",
    "# NO ELIMINO 'price', 'MISC', PUEDE SERVIR PARA LAS RECOMENDACIONES\n",
    "# Por ahora conservo latitude y longitude porque puede servirnos para mostrar locales en el dashboard\n",
    "\n",
    "gm_sitios = gm_sitios.drop(columns=['description', 'state', 'relative_results', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciono las filas que tienen pizza restaurant\n",
    "# df_mt_sitios_pizza = df_mt_sitios[df_mt_sitios['category'].apply(lambda x: 'Pizza restaurant' in x)]\n",
    "# TypeError: argument of type 'NoneType' is not iterable\n",
    "\n",
    "gm_sitios_pizza = gm_sitios[gm_sitios['category'].apply(lambda x: isinstance(x, list) and 'Pizza restaurant' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract state (two uppercase letters) using regex\n",
    "gm_sitios_pizza['state'] = gm_sitios_pizza['address'].str.extract(r',\\s*([A-Z]{2})\\s*\\d{5}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciono las pizzerias de NJ y NY\n",
    "gm_sitios_pizza_NJNY = gm_sitios_pizza[gm_sitios_pizza['state'].isin(['NJ', 'NY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en algunas direcciones dice ', United States' al final / Remove ', United States' if present\n",
    "gm_sitios_pizza_NJNY['cleaned_address'] = [addr.replace(\", United States\", \"\") for addr in gm_sitios_pizza_NJNY['address']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to extract street address, city, and ZIP code\n",
    "pattern_1 = r'(?P<street_address_temp>.+),\\s*(?P<city>[^,]+),\\s*[A-Z]{2}\\s*(?P<zip_code>\\d{5})$'\n",
    "\n",
    "# Aplicar regex a la columna 'address'\n",
    "df_extracted = gm_sitios_pizza_NJNY['cleaned_address'].str.extract(pattern_1)\n",
    "\n",
    "# Combinar con el DataFrame original\n",
    "gm_sitios_pizza_NJNY = gm_sitios_pizza_NJNY.join(df_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To capture only the part to the right of the comma\n",
    "gm_sitios_pizza_NJNY[\"street_address\"] = gm_sitios_pizza_NJNY[\"street_address_temp\"].str.split(\",\", n=1).str[1].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimino las columnas que ya no usamos\n",
    "# df.drop(['Column1', 'Columns2'], axis=1)\n",
    "gm_sitios_pizza_NJNY.drop(['address', 'cleaned_address', 'street_address_temp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elimino los duplicados manteniendo la primera instancia\n",
    "gm_sitios_pizza_NJNY.drop_duplicates(subset=['gmap_id'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary format para extraer los horarios por dia de cada local. La columna es una lista de listas\n",
    "gm_sitios_pizza_NJNY['hours_dict'] = gm_sitios_pizza_NJNY['hours'].apply(lambda x: dict(x) if isinstance(x, list) else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand 'hours_dict' into separate columns (one per day)\n",
    "hours_expanded = gm_sitios_pizza_NJNY['hours_dict'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge expanded columns back into original DataFrame\n",
    "gm_sitios_pizza_NJNY = pd.concat([gm_sitios_pizza_NJNY, hours_expanded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazar '₩', '₩₩' por $ o $$ - esta forma de hacerlo debe ser muy lenta\n",
    "#replacements = str.maketrans({\"h\": \"H\", \"e\": \"E\", \"o\": \"O\"})\n",
    "#res = s.translate(replacements)\n",
    "# string keys in translate table must be of length 1\n",
    "gm_sitios_pizza_NJNY['price'] = (\n",
    "    gm_sitios_pizza_NJNY['price']\n",
    "    .str.replace(\"₩₩\", \"$$\", regex=False)  # Primero reemplaza ₩₩ por $$\n",
    "    .str.replace(\"₩\", \"$\", regex=False)    # Luego reemplaza ₩ por $\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completo con un diccionario vacios las filas que tienen la columna MISC en NaN\n",
    "gm_sitios_pizza_NJNY['MISC'] = gm_sitios_pizza_NJNY['MISC'].apply(lambda x: {} if pd.isnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand 'MISC' into separate columns (one per iem)\n",
    "MISC_expanded = gm_sitios_pizza_NJNY['MISC'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge expanded columns back into original DataFrame\n",
    "# creo que se concatena por los indices. Por eso no necesita que haya una columna en comun\n",
    "gm_sitios_pizza_NJNY = pd.concat([gm_sitios_pizza_NJNY, MISC_expanded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['Column1', 'Columns2'], axis=1)\n",
    "# elimino columnas que ya no necesito\n",
    "gm_sitios_pizza_NJNY.drop(['category', 'MISC', 'hours', 'hours_dict', 'Offerings', 'Dining options', 'Health & safety', 'Accessibility', 'Crowd', 'Planning', \n",
    "'Payments', 'Highlights', 'From the business', 'Health and safety'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por ahora elimino tambien las columnas que luego usare para dummies porque no puedo seleccionarlas \n",
    "# cuando separo las filas de sitios con drop.duplicate porque son listas\n",
    "gm_sitios_pizza_NJNY.drop(['Service options', 'Amenities', 'Atmosphere', 'Popular for'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leo los datasets de reviews completos\n",
    "df_rev_NJ = pd.read_parquet('datos/gm_raw_parquet/gm_review_NJ_raw.parquet' , engine='fastparquet')\n",
    "df_rev_NY = pd.read_parquet('datos/gm_raw_parquet/gm_review_NY_raw.parquet' , engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos, por ahora, name, pics y resp. no usaremos esas columnas\n",
    "df_rev_NJ = df_rev_NJ.drop(columns=['name', 'pics', 'resp'])\n",
    "df_rev_NY = df_rev_NY.drop(columns=['name', 'pics', 'resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar los duplicados manteniendo la primera instancia\n",
    "df_rev_NJ = df_rev_NJ.drop_duplicates(subset=['user_id', 'gmap_id', 'time'], keep='first')\n",
    "df_rev_NY = df_rev_NY.drop_duplicates(subset=['user_id', 'gmap_id', 'time'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir columna time en int64 a datetime\n",
    "df_rev_NJ['date'] = pd.to_datetime(df_rev_NJ['time'], unit='ms')\n",
    "df_rev_NY['date'] = pd.to_datetime(df_rev_NY['time'], unit='ms')\n",
    "\n",
    "# Eliminamos la columan time\n",
    "df_rev_NJ = df_rev_NJ.drop(columns=['time'])\n",
    "df_rev_NY = df_rev_NY.drop(columns=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uno los dataframes de reviews hacer join con el dataframe de sitios /\n",
    "# asi solo conservare los locales para los que haya reviews y las reviews para las que conozcamos los locales\n",
    "df_rev_NJNY = pd.concat([df_rev_NJ, df_rev_NY], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge entre los dos dataframe . Luego guardare solo los que tengan datos en ambos.\n",
    "gm_NJNY = pd.merge(gm_sitios_pizza_NJNY, df_rev_NJNY, how='inner', on='gmap_id') # tengo que hacer inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vuelvo a separar los archivos para dejarlos listos para la base de datos\n",
    "# unique_age_city = df[['Age', 'City']].drop_duplicates()\n",
    "# por ahora no seleccionamos las columnas 'Service options', 'Amenities', 'Atmosphere', 'Popular for' porque las listas no permite drop.duplicates() \n",
    "gm_sitios_NJNY = gm_NJNY[['gmap_id', 'name', 'street_address', 'city', 'state', 'zip_code',   'latitude','longitude','avg_rating','num_of_reviews','price',\n",
    "                   'Monday','Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', ]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora selecciono las reviews\n",
    "gm_rev_NJNY = gm_NJNY[['gmap_id', 'user_id', 'date', 'rating' , 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estos son los archivos que hay que cargar en las tablas para dashboard y modelos de ML\n",
    "\n",
    "# guardo gm_sitios_NJNY\n",
    "gm_sitios_NJNY.to_parquet('datos/gm_raw_parquet/gm_sitios_NJNY_202502182030.parquet' , engine='fastparquet')\n",
    "\n",
    "# guardo gm_rev_NJNY\n",
    "gm_rev_NJNY.to_parquet('datos/gm_raw_parquet/gm_rev_NJNY_202502182030.parquet' , engine='fastparquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
